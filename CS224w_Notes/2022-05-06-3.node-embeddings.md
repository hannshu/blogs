---
categories: CS224w_Notes
---

图机器学习通过表示学习来自动的学习特征，不需要进行特征工程  
embedding的主要目的是降维，将稀疏的特征转换为稠密的特征  

# graph representation learning
将图中的每个结点都映射成一个d维的向量表示这个点的特征  
## encode
目标: 将图中的每个点转换为一个d维向量  
优化: 使结点在embedding space中的相似度和图中的相似度相似  
对于结点u, v，在图中定义一个similarity(u, v) $$\approx$$ embedding space中的相似度(需要自己定义，比如得到的两向量的点乘...)  
Shallow encoder: embedding lookup    
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141922268.png)  
<b>Z</b>v只是查询矩阵<b>Z</b>中v结点的信息  
<b>Z</b>是一个每列都是d维的一个n列的矩阵，其中每列保存的是图中一个结点的embedding vector(需要通过训练得到)  

# random walk approach
## word2vec
[code](https://github.com/dav/word2vec/blob/master/src/word2vec.c)  
用特征向量表示单词，且每两个词向量可以通过计算余弦相似度表示他们之间的关系  

### skip-gram model
用一个词预测周围的词(窗口大小内的)  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141922166.png)  
此处的矩阵<b>W</b>就是上面所说到的embedding vector组成的矩阵，如果单词使用onehot编码，则只需要查找对应的那一列就是layer层的输入参数  
softmax: 用于将输出的结果y向量中每个值进行归一化  
将每个(input, output)加入网络中训练，最终得到矩阵<b>W</b>  

===

对于每一个中心词，需要做的是使 $$\prod_{中心词 \in 每次滑动窗口后的中心词} \prod_{word \in 窗口中的其他单词} P(word|中心词) $$ 最大化(类似最大似然估计)  
模型的目的可以简单表示为: $$  {\argmax_{词向量矩阵}}\sum_{word \in 文本} \sum_{c \in 中心词周围的滑动窗口} \log P(c|word) $$  
下面的问题是如何表达P(c|word) -> 希望如果c和word挨得比较近时P(c|word)比较大(这种情况下c和word应该形似度比较大) -> 通过softmax的方式表达  
这一的P(c|word)其实与向量的内积(向量夹角$ \cos(\theta) $)正相关  
$$ P(c|word) = \frac{e^{词向量矩阵_{word} * 词向量矩阵_{c}}}{\sum_{v \in 文本} e^{词向量矩阵_{word} * 词向量矩阵_{v}}} $$  
(这里如果细分的话可以将词向量矩阵分为u和v两部分，当这个单词作为中心词加入计算时使用u矩阵，作为滑动窗口中的其他词出现时使用v矩阵，训练完成后通过u + v得到最终的词向量矩阵)  
最终表达式可得:  
$$  {\argmax_{词向量矩阵(U; V)}}\sum_{word \in 文本} \sum_{c \in 中心词周围的滑动窗口} (U_{word} * V_{c} - \log \sum_{c' \in 文本} e^{U_{word} * V_{c'}}) $$  
由于词库较大，在计算softmax的分母时计算量非常大，使用negative sampling或hierarchical softmax解决    

### negative sampling 
将问题修改为: 在一个上下文中，使P(D=1|w1, w2)尽可能大，P(D=0|w1, w2)尽可能小; 反之同理(将原问题修改为一个类似二分类的问题)  
目标函数修改为: 
$$
{\argmax_{词向量矩阵(U; V)}}\prod_{(w, c) \in 文本} P(D=1|w, c) * \prod_{(w, c) \notin 文本} P(D=0|w, c)
$$
这里(w, c)表示词对w(中心词)和c(上下文词)是否有出现在同一个滑动窗口中  
化简表达式可得:  
$$
{\argmax_{词向量矩阵(U; V)}}\sum_{(w, c) \in 正样本} \log \sigma (U_w * V_c) + \sum_{(w, c) \in 负样本} \log \sigma (-U_w * V_c)
$$  
其中$ \sigma (U_w * V_c) = \frac{1}{1 + e^{-U_w * V_c}} $  
这里出现一个问题: 正样本数量 << 负样本数量 -> 采样一部分负样本进行计算  
最终表达式:  
$$
{\argmax_{词向量矩阵(U; V)}}\sum_{(w, c) \in 正样本} (\log \sigma (U_w * V_c) + \sum_{c' \in w为中心词的k个负样本} \log \sigma (-U_w * V_{c'}))
$$  

### CBOW
与skip-gram类似，这里是将选中的词作为output，将滑动窗口中的其他词相叠加作为input  
使用这种方法训练得到embedding vector的矩阵  

## deepwalk
在图中沿着边随机游走生成结点序列，然后通过word2vec的方法优化embedding vector矩阵  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141923899.png)

## 梯度下降
步骤: 每走到一个位置，求解当前位置的梯度(求解当前变量的偏导数) -> 沿梯度方向移动(更新当前变量的值) -> 继续求解梯度...直到得到最优解  
如对于上述的negative sampling:  
$$
f = {\argmax_{词向量矩阵(U; V)}}\sum_{(w, c) \in 正样本} (\log \sigma (U_w * V_c) + \sum_{c' \in w为中心词的k个负样本} \log \sigma (-U_w * V_{c'}))  \\  
\frac{\partial{f}}{\partial{V_c}} = \sigma (-U_w * V_c) * U_w => V_c = V_c + \eta * \frac{\partial{f}}{\partial{V_c}} \\   
\frac{\partial{f}}{\partial{V_{c'}}} = -\sigma (U_w * V_{c'}) * U_w => V_{c'} = V_{c'} + \eta * \frac{\partial{f}}{\partial{V_{c'}}}  \\   
\frac{\partial{f}}{\partial{U_w}} = \sigma (-U_w * V_c) * V_c + \sum_{c' \in w为中心词的k个负样本} -\sigma (U_w * V_{c'}) * V_{c'} => U_w = U_w + \eta * \frac{\partial{f}}{\partial{U_w}}  
$$
(这里写加法是因为是用过argmax使得到的f最大，如果是argmin则应该使用减法)  


## node2vec
homophily(同质性): 结点与周围结点应比较相似(bfs) -> return parameters p  
structural equivalence(结构等价性): 处于相似位置的结点应比较相似(dfs) -> in-out parameter q  

每次游走时都是有策略的:  
当前结点为v，上一步为t，则下一步走到结点x(t -> v(cur) -> x)的概率为:  
$$\pi_{vx} = \alpha_{pq}(t, x) * \omega_{vx}$$  
其中
$$ \alpha_{pq}(t, x)=\left\{
\begin{aligned}
1/p & & t与x的距离为0(即返回t点) \\
1   & & t与x的距离为1 \\
1/q & & t与x的距离为2
\end{aligned}
\right.
$$  
算法：  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141923152.png)  
注：这里的 $$\gamma$$ 是随机游走的次数  
同时在随机游走的过程中，可以从生成的序列中截取出新的序列以提高效率  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141924316.png)  
如果q值比较小，则序列会优先沿着深度方向遍历  
如果p值比较小，则序列会优先沿着广度方向遍历  

# embedding entire graphs
## 法1
Run a standard graph embedding technique on the (sub)graph 𝐺. Then just sum (or average) the node embeddings in the (sub)graph 𝐺  
直接对图中所有结点的embedding vector相加得到图的embedding vector  

## 法2
Introduce a “virtual node” to represent the (sub)graph and run a standard graph embedding technique.  
用一个虚拟结点代替子图的所有结点，将其他与子图链接的结点全都连接到这个结点上即可  

## anonymous walk embedding
1. 将每次的walk保存为一种状态，如下图：  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141924018.png)  
给定步长l后会产生多种匿名的序列，记录出现的频次作为embedding vector  
通过以下公式计算需要模拟的walk的次数：  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141924933.png)

2. 给每个匿名walk学习embedding vector   
-> (没看懂)  

# embedding应用
聚类node  
分类node  
连接预测: 通过 $$z_i和z_j$$ 来预测i和j间是否存在边  
分类图
