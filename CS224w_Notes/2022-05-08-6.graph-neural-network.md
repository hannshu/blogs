---
categories: CS224w_Notes
---

# shallow encoder的缺点
1. 需要n * d的空间(用于存放结点的embedding vector)，每个结点有独立的embedding，结点间没有共享的变量  
2. 不同图、结点间不能使用相同的embedding space，需要重新训练  
3. 没有用到结点的特征  

# deep graph encoders
encoder -> 对图进行的多层次的非线性的转换  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141934323.png)  

## 应用
node classification  
link prediction  
community detection  
network similarity  

# 深度学习基本概念
## basic deep learning 
formulate the task as an optimization problem  
$$ \min_{\Theta} L(y, f(x)) $$
$$\Theta$$是我们希望优化的值(可以是一个标量，一个向量，一个矩阵...)  
L是损失函数，用来量化真实值y和预测值f(x)的关系  
[损失函数](https://pytorch.org/docs/stable/nn.html#loss-functions)有很多种，他们适用于不同的任务  

## gradient vector
$$ \nabla_{\Theta}L = (\frac{\partial L}{\partial \Theta_1}, \frac{\partial L}{\partial \Theta_2}, ...) $$  
梯度向量给出了$$\Theta$$增大/减小的方向 -> $$\Theta = \Theta \pm \eta * \nabla_{\Theta}L$$  
理想状态下: gradient vector = 0  
在现实状况中，如果gradient vector已经达到了很小，则不再继续训练  

## stochastic gradient descent(SGD)  
因为现代计算中的数据集数据量很大，如果对每个x都计算梯度会很浪费时间  
solution: At every step, pick a different minibatch 𝓑 containing a subset of the dataset, use it as input 𝒙  
即损失函数是由一批数据加权获得的  
- batch: 用于评估梯度的子集  
- batch size: the number of data points in a minibatch  
- iteration: 1 step of SGD on a minibatch(iterations = dataset size / batch size)  
- Epoch: one full pass over the dataset  

梯度的估计可以表示为 $$g = \frac{1}{m'}\nabla\sum_{i = 1}^{m'}L(x^{(i)}, y^{(i)}, \Theta)$$  
每次只随机采取m'(batch size)个数据，然后训练，最后加权计算出梯度(不会每次训练都更新$$\Theta$$，而是一批训练节后再更新)

## back propagation 
前向传播: 网络通过接收input: x然后产生output: f(x)的过程中，信息通过网络向前流动  
反向传播: 将产生的代价函数的信息通过网络向后流动，用来计算梯度  

使用复合函数的链式法则来求解每个位置的参数的偏导数  
假设网络有两层，则有f(x) = W2(W1 * x)，$$\Theta = \{W_1, W_2\}$$  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141935451.png)  

## non-linearity
经过线性变换后，通常添加一个非线性变换的运算来增加模型的表现力  
常用非线性变换函数:  
rectified linear unit(ReLU): ReLU(x) = max(x, 0)  
sigmoid: $$\sigma (x) = \frac{1}{1 + e^{-x}}$$  

## multi-layer perceptron(MLP)
MLP的每一层由一次线性变换以及一次非线性变换组成  
ex. $$x^{(l + 1)} = \sigma(W_lx^{(l)} + b_l)$$  

# deep learning for graph
- 最简单的想法:  
直接将邻接矩阵和结点特征连接并放入神经网络中  
-> 随着结点的增加，这种方式会导致增加非常多的参数  
-> 模型不能适用于不同结点个数的图(模型只能针对这一张图)  
-> 对于邻接矩阵中结点的排序十分敏感  

## graph neural network
参考卷积神经网络的方法: 将一小片像素点的信息结合起来，向下传递  
对于图而言: 将结点邻居的信息传递到中心点上并结合，然后传递到下一层  
transform 'message' hi from neighbors: Wi * hi -> add them up: $$\sum_iW_i * h_i$$  

- determine node computation graph  
- propagate and transform information  

![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141936243.png)  
如图A通过他的邻居获取信息，然后这些邻居再通过他们的邻居获取信息  
每个结点都会有自己独特的计算图 -> 需要同时计算每个结点的网络      
通过神经网络的层数k来限制结点邻居的范围(layer-k表示会使用到结点k层的邻居)  
每层的结点将信息传到下一层，下一层整合上一层的信息，并添加自己的信息，然后向下传递，直到最后一层  

### neighborhood aggregation
基础方法: 将从上一层结点取来的信息先进行求和取平均，然后再放入一个神经网络中(进行线性和非线性变换)  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141936424.png)  
注: 对于整张网络每个相同的layer层均使用同一个W和B  

-> GNN: 提取A的图的结构信息  

