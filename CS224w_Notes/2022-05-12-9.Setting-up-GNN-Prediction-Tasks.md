---
categories: CS224w_Notes
---

# GNN的表达效果
## 计算图存在的问题
如果两个结点的计算图相同(两个结点是对称的)，那么如果所有结点的特征向量相同的情况下，无法区分这两个结点  

## 局部结构信息
一般来说，不同的局部邻居会得到不同的计算图 -> 能区分不同的结点  

计算图在实质上就是以结点为根的一棵子树(rooted subtree structure)  
表示能力优秀的GNN模型可以将不同的rooted subtree structure映射到结点嵌入矩阵中不同的嵌入向量上  

### 单射(injective)
将不同自变量x映射到不同的因变量y中  
每个x都对应一个独一无二的y  

表示能力优秀的GNN模型就应该单射地映射子树到节点嵌入(即不同的子树映射为不同的嵌入)  
表示能力优秀的GNN就是每一步都使用单射邻居聚合函数(保留全部信息)，把不同的邻居映射到不同的嵌入上  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141945355.png)  
-> GNN can fully distinguish different subtree structures if every step of its neighbor aggregation is injective.  


## 比较聚合函数的表示能力
聚合函数可以抽象为一个对multi-set的函数(集合中可以有不同数量、不同特征的无序的结点) -> 经过聚合函数后到达父结点    
### 比较GNN和GraphSAGE的聚合函数  
- GCN: 平均池化  
每个结点先线性变化 + ReLU -> 平均池化  
无法区分有相同比例不同个数的相同结点的multi-set(如无法区分一黄一蓝和两黄两蓝 -> 因为取平均值后结果相同)  
-> 平均池化不是单射函数  

- GraphSAGE: 最大池化  
每个结点先MLP -> 最大池化  
无法区分有相同元素的multi-set(最大池化一定会取这几个元素中最大的那个，无论这个元素在集合中有几个)  
-> 最大池化不是单射函数  

### 设计一个单射函数
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141945337.png)  
-> 使用一个MLP: 只有一个隐藏层的MLP只要隐藏层维度够宽，并有合适的非线性函数(包括ReLU和sigmoid)，就可以任意精度逼近任何连续函数  
$$MLP_{\Phi}(\sum_{x \in S}MLP_f(x))$$  
在实践中，MLP隐藏层的维度在100 - 500之间就足够了  

## GIN
使用上述的MLP做聚合函数  
GIN网络中中所有邻居聚合函数都是单射函数的  
### 实质上GIN是一个神经网络版本的WL kernel
ML kernel在进行聚合时使用的函数为: $$c^{(k + 1)}(v) = HASH(c^{(k)}(v), \{ c^{(k)}(u) \}_{u \in N(v)})$$  
这个函数用于将邻居结点的id和结点本身的id聚合后求哈希，其实本质上也是进行了非线性变换  
经过多次迭代之后，如果两网络内各结点的id相同，则说明两网络同构，否则异构  

### 具体实现
$$ c^{(k + 1)}(v) = GINConv(c^{(k)}(v), \{ c^{(k)}(u) \}_{u \in N(v)}) \\
= MLP_{\Phi}((1 + \epsilon) * MLP_f(c^{(k)}(v)) + \sum_{u \in N(v)} MLP_f(c^{(k)}(u))) $$  
其中:  
$$c^{(k + 1)}(v)$$是聚合后结点v的特征向量  
$$c^{(k)}(v)$$是v结点当前的特征向量  
$$c^{(k)}(u)$$是v的邻居结点u的特征向量  
$$\epsilon$$是一个需要学习的常量  

### 步骤
为每个结点v初始化$$c^{(0)}(v)$$  
通过GINConv()反复迭代  
迭代k次后，$$c^{(k)}(v)$$可以表示为k跳的邻居信息的结合  

## 与ML Kernel相比的优点
节点嵌入是低维的，因此可以捕获到细粒度的节点相似性  
GINConv()的参数可被学习得到并应用于下流任务  



