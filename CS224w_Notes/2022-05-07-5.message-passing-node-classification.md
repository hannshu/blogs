---
categories: CS224w_Notes
---

如何预测结点的标签  

# collective classification
结点附近的结点更倾向于是同一种label  

homophily: 相似的人更倾向于存在联系  
(有同种特性的人更倾向于是有联系的)  
influence: 社会关系会影响一个人的特征  
confounding: 外部环境会影响性格和社会关系  

通常一个结点的特征取决于:  
1. 当前结点的特征  
2. 当前结点邻居结点的label  
3. 当前结点的邻居结点的特征  

马尔可夫假设: the label $$Y_v$$ of one node v depends on the labels of its neighbors $$N_v$$  
$$ P(Y_v) = P(Y_v|N_v) $$  

步骤:  
1. local classifier: Predicts label based on node attributes/features(不使用结点信息)  
2. relational classifier: Learns a classifier to label one node based on the labels and/or attributes of its neighbors(使用网络信息)  
3. collective inference: (使用上面预测得到的结点信息)Iterate until the inconsistency between neighboring labels is minimized(使用网络结构信息)  

# probabilistic relational classifier
初始化: 有类别的结点初始化为label，没类别的结点初始化为0.5  
使用结点v的所有邻居结点u以下公式反复迭代计算:  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141932207.png)  
(其中 $$A_{v, u}$$ 是结点u, v之间的权值，无权图则为1)  
迭代结束后P(v) > 0.5 => label(v) = 1，反之为0  

-> 不能保证算法最后收敛  
-> 算法没有使用到结点自身的信息  

# iterative classification
Classify node 𝑣 based on its attributes 𝒇𝒗 as well as labels 𝒛𝒗 of neighbor set 𝑵𝒗.  

## 思路
训练两个分类器，然后利用分类器迭代预测结点标签:  
1. $$\phi_1 (f_v)$$ = 基于结点特征$$f_v$$预测的结点标签  
2. $$\phi_2 (f_v, z_v)$$ = 基于结点特征$$f_v$$和 通过v的邻居结点的标签得到的汇总信息$$z_v$$ 共同预测得到标签  
(这里的分类器可以是任意分类器)  

## 步骤
### 阶段1: 仅基于结点属性的分类(训练分类器)
对于有标签的训练集，训练上述的两个分类器  

### 阶段2: 迭代直到收敛(测试过程)
在测试集中，根据 $$\phi_1 (f_v)$$ 先设置好每个结点的标签  
然后通过每个结点v的邻居结点计算 $$z_v$$ 并用于 $$\phi_2 (f_v, z_v)$$ 预测标签，不断重复迭代  
这种方法不能保证收敛性  

# loop belief classification
## 概率图模型
数据是一个高维的随机变量的概率分布P(x1, x2, ..., xp)  
sum rule: $$P(x_1) = \int P(x_1, x_2)dx_2$$ -> 边缘概率  
product rule: P(x1, x2) = P(x1) * P(x2 | x1) = P(x2) * P(x1 | x2)  
chain rule: $$P(x_1, x_2, ..., x_p) = \sum_{i = 1}^p P(x_i | x_1, x_2, ..., x_{i - 1})$$  
bayesian rule: $$P(x_1 | x_2) = \frac{P(x_1, x_2)}{\int P(x_1, x_2)dx_1} = \frac{P(x_2) * P(x_1 | x_2)}{\int P(x_1, x_2)dx_1}$$
-> 维度高时计算复杂(联合概率P(x1, x2, ..., xp)计算量太大)  
简化模型:  
1. 假设每个维度之间相互独立  
$$P(x_1, x_2, ..., x_p) = \prod_{i = 1}^pP(x_i)$$ -> 朴素贝叶斯: $$P(x | y) = \prod_{i = 1}^pP(x_i | y)$$  
2. 马尔可夫性: 在给定当前状况下，将来的状况与以前的状况相互独立，只与当前的状态与关  
3. 条件独立性假设: 给定三个事件x, y, z，如果P(x, y | z) = P(x | z) * P(y | z)，则x, y, z条件独立 -> 当z发生时，x发生与否和y发生与否之间是无关的  

### 概率图
1. 表示(representation): 有向图、无向图  
2. 推断(inference): 精确推断、近似推断  
3. 学习(learning): 参数学习、结构学习  

### 贝叶斯网络
假设一个网络中存在三个结点(随机变量): a, b, c  
- 如果存在边: a -> b, a -> c, b -> c  
则P(a, b, c) = P(a) * P(b | a) * P(c | a, b)  
这种情况下三个随机岸边两之间都有箭头，则说明彼此都有联系，不存在独立的变量  

条件独立的情况:  
- tail-to-tail  
如果存在边: c -> a, c -> b  
即只有c指向a, b的边  
则P(a, b, c) = P(c) * P(a | c) * P(b | c)  
P(a, b | c) = P(a, b, c) / P(c) = P(a | c) * P(b | c)  

- head-to-head  
如果存在边: a -> c, b -> c  
即只有指向c的边  
则P(a, b, c) = P(a) * P(b) * P(c | a, b) = P(a, b) * P(c | a, b)  

- head-to-tail  
如果存在边: a -> c -> b  
P(a, b, c) = P(a) * P(c | a) * P(b | c)  
在c发生的条件下有:  
P(a, b | c) =  P(a, b, c) / P(c) = P(a) * P(c | a) * P(b | c) / P(c) = P(c) * P(a | c) * P(b | c) / P(c) = P(a | c) * P(b | c)  
则可知在c发生的条件下，a, b独立  

通过以上可以推得，如果存在网络: x1 -> x2 -> ... -> xk  
在xi发生的条件下，$$x_{i + 1}$$的分布与x1, x2, ..., $$x_{i - 1}$$条件独立 -> $$x_{i + 1}$$的分布只与xi有关  
即$$P(x_{i + 1} | x_1, x_2, ..., x_i) = P(x_{i + 1} | x_i)$$  

### 马尔可夫场
对于无向图，将结点划分成组  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141933056.png)  
如图集合A和B之间如果所有路径都通过集合C中的结点，则表示A和B被C所阻隔 -> 条件独立  

团块：图中结点的子集，子集的节点之间一定两两直接连接(间接连接起来的不算)。团块中的节点集合是全连接的  
最大团块：将图中任何一个其它节点接到该到团块中就会破坏团块的性质(不再两两连接)  

首先将图中结点分为若干个极大团块  
则有$$P(x_1, x_2, ..., x_k) = \frac{1}{Z} * \prod_{i = 1}^p \Psi_{C_i}(x_{C_i})$$  
其中势函数 $$\Psi_{C_i}(x_{C_i}) = e^{-E(x_{C_i})}$$  
Z用于归一化的 $$Z = \sum_x \prod_C \Psi_C(x_C)$$  
与有向图类似，马尔可夫场中k位置的值只与其周围位置的值有关  

### 因子图





