---
categories: CS224w_Notes
---

# 异构图(heterogeneous graphs)
图中有多种类型的结点、边  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141947973.png)  
通过改良GCN使其可以处理异构图的问题  

## relational GCNs
不同种类的边  
传统的GCN的layer:  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141948377.png)  
由于存在不同种类的关系，在建立计算图的时候，每个结点传递信息所经过的神经网络应按照边的类型进行区分(即个layer层中不再只使用同一个变换矩阵W，而是依照边的不同使用不同的变换矩阵)  
RGCN:  
$$h_v^{(l + 1)} = \sigma(\sum_{r \in R} \sum_{u \in N_v^{r}} \frac{1}{c_{v, r}}W_r^{(l)}h_u^{(l)} + W_0^{(l)}h_v^{(l)})$$
- message: 
each neighbor of a given relation:  
$$ m_{u, r}^{(l)} = \frac{1}{c_{v, r}}W_r^{(l)}h_u^{(l)} $$
self-loop:  
$$ m_v^{(l)} = W_0^{(l)}h_v^{(l)} $$
- aggregation:  
$$h_v^{(l + 1)} = \sigma(Sum(\{ m_{u, r}^{(l)}, u \in \{ N(v) \} \cup \{ v \} \}))$$  
每层中的转换矩阵$$W_r^{(l)}$$应该是本层特征向量维度和下一层特征向量维度的乘积$$d^{(l + 1)} * d^{(l)}$$  
这种情况下会导致出现非常多的参数和矩阵，致使模型无法进行训练  

### block diagonal matrices
将权重矩阵变的稀疏  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141949924.png)  
如图，其实是减少了隐藏层的路径，从而起到减少权重矩阵大小的目的  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141949757.jpeg)  

### basis learning
在不同关系之间共享权重  
$$ W_r = \sum_{b = 1}^B a_{rb} * V_b $$  
其中:  
$$ V_b$$是共享矩阵，在每层layer上共享(basis matrices)  
$$a_{rb}$$是$$ V_b$$关于这个关系的重要性权重  
这样对于每个关系而言，只需要学习其重要性权重$$\{ a_{rb} \}_{b = 1}^B$$即可  

### 结点预测  
先计算出每个结点的node embedding  
基于嵌入向量可以用于构建prediction head(如共k类，通过对node embedding进行线性/非线性变换后，得到prediction head，然后通过softmax得到最大概率的类)  

### 连接预测
-> 难点在于边有非常多种，不应该轻易的拆分边(有些关系出现的概率小，可能不能平均分到每种集合中)  
如图，通过两结点的node embedding以及打分函数来确定两结点间存在结点的概率  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141950890.png)  
- 训练阶段:  
用RGCN给training supervision edge(E, r3, A)进行打分  
建立一个负样本(negative edge)来扰乱预测边，比如在(B, E)之间建立边  
使用RGCN对边(B, E)打分  
优化交叉熵函数(CE)使得正样本(E, A)的得分最大，负样本(B, E)的得分最小  
$$l = - \log (\sigma (f_{r_3}(h_E, h_A))) - \log (1 - \sigma (f_{r_3}(h_E, h_B)))$$  

- 验证阶段:  
如果两结点间存在边，则 这条边的评分 应该比 从出发结点到其他没有和这个结点有边的其他结点组成的边 的评分都高  
ex. 验证(D, E)是否存在边, 则$$Score(D, E) > Score(D, v)$$其中结点v应该是与D不存在边的结点  

计算需要验证的边的得分  
计算从出发边到其他结点的边(还没有边的结点)的得分  
将他们排名，得到验证边的排名RK  
判断是否存在边:  
1. 如果$$RK \leq k$$(其中k是正样本的ranking)，则存在边  
2. 如果1 / RK大于某阈值，则存在边  

-> 前面讲过的所有GNN模型都可以改进成能解决异构图的模型  

# knowledge graphs
使用图来存储某个领域的知识  
捕获实体entity(节点)、类型(节点标签)、关系relationship(边)  
是异构图的一种 -> 表示方法$$triples(h, r, t)$$  
h -> head, r -> relation, t -> tail  
现有的公开知识图谱有一个共同的特点: 大且不完整  
-> KG completion: 用于预测知识图谱中可能存在但缺失了的边  
这种图与传统的图不同在于已知结点和边，只是想确定给定的结点对之间是否存在给定的关系($$r_i$$类型的边)    
一般使用shallow embedding的方法求得embedding space(h, r)   

## 如何求embedding space(h, r)
-> 这里是为了进行KG Completion，通过head和relation来预测tail  

### TransE
一般情况下，希望$$h + r \approx t$$  
Scoring function: $$f_r(h, t) = -\vert\vert h + r - t \vert\vert$$  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141951626.png)  

关系(边)的分类:  
1. symmetric(antisymmetric) relations:
$$r(h, t) => r(t, h) \\
r(h, t) => \neg r(t, h) \\
 \forall h, t \in Graph
$$
2. inverse relations:
$$r_2(h, t) => r_1(t, h)$$
3. composition(transitive) relations:
$$r_1(x, y) \wedge r_2(y, z) => r_3(x, z) \forall x, y, z \in Graph$$
4. 1-to-N relations:  
$$ r(h, t_1), r(h, t_2), ..., r(h, t_n) $$ are all True  

TransE可以捕捉:  
antisymmetric: h + r = t, t + r != h  
inverse relations: r1 = -r2   
composition: r3 = r1 + r2  
无法捕捉剩余的两种类型的边  

### TransR
为了解决TransE不能捕捉的两种边  
每个结点由结点空间$$R^d$$中的一个向量表示  
每个关系(不同的边)由一个关系空间中的向量r和对应关系的关系矩阵$$M_r$$组成 ($$r \in R^k \  with \  M_r \in R^{k * d}$$)  
在预测时先将结点转换到关系空间中，再判断结点间是否存在关系  
$$
h_{\perp} = M_r * h, \  t_{\perp} = M_r * t \\
f_r(h, t) = - ||h_{\perp} + r - t_{\perp}||
$$
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141952593.png)  

symmetric: r = 0, $$h_{\perp} = M_r * h = M_r * t = t_{\perp}$$  
只需要学习$$M_r$$使其将h和t映射到关系空间中同一个位置即可  
1-to-N: $$t_{\perp} = M_r * t_1 = M_r * t_2$$  
只需要学习$$M_r$$使其将$$t_i$$映射到关系空间中同一个位置即可  
(对于TransE可以捕捉到关系，只需要保持$$M_r$$相同即可)  

-> TransR无法捕捉composition relations:  
确定每条边时是通过在该关系的关系空间中考察h, r, t，但是我们无法确定不同关系空间之间是否有关  

### Bilinear Modeling(DistMult)
修改打分函数Scoring function: $$f_r(h, t) = <h, r, t> = \sum_i h_i * r_i * t_i$$  
如果关系成立，则打分函数可以给出很高的分数，反之分数应该很低  
将h * r想象成一个向量，则在超平面中，如果这个向量与r相似(即相近，夹角小)，则得到的值应该很大  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141953461.png)  

无法捕捉:  
antisymmetric: 因为乘法存在交换律，所以 h * r * t 和 t * r * h 完全相同    
inverse relations: 因为这会使r1 = r2，这不符合知识图谱的语义(会变为symmetric relations)  
composition: 两个超平面的并集就不再是一个超平面  

### ComplEx
将每个结点、关系表示为一个复数向量  
打分函数与DistMult相同，是对实部部分的运算  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141954743.png)  

因为与DistMult使用相同打分函数，complEx也不能捕捉composition关系  


![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141954496.png)  
通过实际数据需要来确定使用那种[模型](https://openreview.net/forum?id=HkgEQnRqYQ&noteId=HJlFFR7167)  

