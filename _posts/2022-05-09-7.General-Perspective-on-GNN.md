---
categories: CS224w_Notes
---

# GNN的数学表示
首先基于结点的邻居结点们建立计算图(computation graph)  
然后将计算图看作一个网络，每次从结点的邻居按边传递信息并聚合直到中心结点  
-> 需要学习的是结点聚合过程中的参数W和B  

## 如何定义GNN的每层layer
GNN layer = message + aggregation  
-> 用于压缩来自邻居结点的信息  

## message function
$$m_u^{(l)} = MSG^{(l)}(h_u^{(l - 1)})$$  
本层的每个结点都创建一条信息，然后发送给下一层结点  

## aggregation
$$h_v^{(l)} = AGG^{(l)}(\{m_u^{(l)}, u \in N(v)\})$$  
将传递到本层的所有结点的信息压缩聚合成一条信息  

-> 这里都没有考虑到本层的结点v的信息$$h_v^{(l - 1)}$$  
1. 计算结点v的信息:  
对于邻居结点，使用 $$m_u^{(l)} = W^{(l)}h_u^{(l - 1)}$$ 计算信息  
对于当前的结点v，使用另一个不同的矩阵 $$m_v^{(l)} = B^{(l)}h_v^{(l - 1)}$$ 计算信息  
2. 在邻居结点的信息聚合完成后，可以再聚合当前结点v的信息:  
$$h_v^{(l)} = CONCAT(AGG^{(l)}(\{m_u^{(l)}, u \in N(v)\}), m_v^{(l)})$$  

在最后，将最终的信息通过一个非线性函数  

# 图神经网络
## GCN
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141937358.png)  

## GraphSAGE
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141937759.png)  
相比于GCN:  
1. 考虑到可以使用多种的聚合函数，而不是只是使用求和取平均的方法  
2. 考虑到了结点v自身的消息，每次需要将邻居结点的信息和结点v自身信息结合  

### l2归一化
在每个layer的最后进行一步归一化处理  
$$ h_v^{(l)} = \frac{h_v^{(l)}}{||h_v^{(l)}||_2}$$
因为向量中的每个参数可能有不同的尺度  

## GAT
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141937342.png)  
对于每个邻居传来的信息，都添加一个权重(attention weight)  
这个权重可以描述邻居结点的重要性  

- 在GCN或是GraphSAGE中，权重$$\alpha_{vu} = \frac{1}{|N(v)|}$$，这种情况下，权重只与结点v有关  
GAT认为在消息传递的过程中，应该更注重于重要的信息，忽略不重要的信息  
这里的注意力权重$$\alpha_{vu}$$也需要在训练中学习得到  

- 计算注意力权重$$\alpha_{vu}$$: $$e_{vu} = a(W^{(l)}h_v^{(l - 1)}, W^{(l)}h_u^{(l - 1)})$$  
通过注意力机制(attention mechanism)a计算出与结点v连接的每个边的attention coefficient $$e_{vu}$$  
然后使用softmax函数使结点v的所有e的和为1，通过softmax函数后的值就是这条边(u, v)的注意力权重  
$$\alpha_{vu} = \frac{e_{vu}}{\sum_{n \in N(v)} e_{vn}}$$

- 注意力机制a: a用于给每个边打分，a可以是多种多样的函数  

### multi-head attention
Stabilizes the learning process of attention mechanism  
使用多个$$\alpha_{vu}$$:  
$$
h_v^{(l)}[1] = \sigma(\sum_{u \in N(v)} \alpha_{vu}^1W^{(l)}h_u^{(l - 1)}) \\
h_v^{(l)}[2] = \sigma(\sum_{u \in N(v)} \alpha_{vu}^2W^{(l)}h_u^{(l - 1)}) \\
h_v^{(l)}[3] = \sigma(\sum_{u \in N(v)} \alpha_{vu}^3W^{(l)}h_u^{(l - 1)}) \\
$$
这里的每个$$\alpha_{vu}^i$$都应该使用不同的a来预测得到，且每个a在初始化时都应该是不相同的随机值    
然后在输出时再将每个不同权重所得到的信息聚合: $$h_v^{(l)} = AGG(h_v^{(l)}[1], h_v^{(l)}[2], h_v^{(l)}[3])$$  

- 注意力机制的优点:  
为每个邻居结点都设置了特殊的权重$$\alpha_{vu}$$  
注意力机制a只关注于邻居结点间的本地网络的性质，所以可以用于给图的其他部分以及其他图使用  

注: a -> b的注意力权重和b -> a的注意力权重不一定相等  

# 定义说明
## batch normalization
Stabilize neural network training  
将一批不太标准的数据统一到指定的格式  
我们在数据处理时常用的是将一组范围差距较大或者单位不同的数据依据一定规则变化到指定的范围之内。
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141938359.png)  
为了防止激活函数使得数据的分布发生偏移，使用batch normalization的方式减轻这种状况  
在每层线性变换之后进行BN -> 再进行非线性变换  

## dropout
prevent overfitting  
在神经网络训练中，按概率p将随机的神经元关闭  
在测试时，使用所有的神经元  

- 在GNN中的使用方法: 在线性变换的过程中，将一些input和output关闭  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141938942.png)  

## activation
Rectified linear unit: ReLU(xi) = max(xi, 0) -> most commonly used  
Sigmoid: $$\sigma(x_i) = \frac{1}{1 + e^{-x_i}}$$ -> 用于需要限制输出结果的大小时(0, 1)  
Parametric ReLU: PReLU(xi) = max(xi, 0) + ai * min(x, 0)  
ai是一个需要训练的参数，这种函数效果优于ReLU  

# stacking layers of a GNN
标准过程:  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141939159.png)  

## smoothing problem
随着网络层数的增加，所有结点的输出值都变相同了    
这主要是因为随着层数的增加，与每个结点有关的结点变多(感受野)，导致原本不同的一些结点也变得看似相同了  
- 应当慎重选择层数:  
确定感受野大小 -> 设定层数稍微大于感受野  

### 如何让有限个layer更具有表现力
1. 让每个layer变得更复杂，如修改W和聚合过程变为 3-layer MLP  
2. 为GNN添加不传递信息的MLP层 -> 将这些层看作是预处理和后处理层 -> 将经典神经网络层和图相结合  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141939309.png)  
3. 如果一定需要很多层，则可以考虑加入跳转连接  
由于上层的layer(离中心结点较远的层)有时能更好的区分不同的结点  
通过跳转连接的方式来增加这些早期结果对后面layer的影响  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141939809.png)  
跳转n次的网络会生成2^n个路径  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141939660.png)  
如图中虽然看上去只是3次跳转，但可以看出，其实其生成了2^3个路径  
![](https://cdn.jsdelivr.net/gh/hannshu/imgs/img/202301141940076.png)  
skip connections也可以跨多层，直接跨到最后一层，在最后一层聚合之前各层的嵌入  

