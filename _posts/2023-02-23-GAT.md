---
categories: GNN
title: GAT
---

不同于GCN使用谱图卷积来归一化邻接矩阵，GAT选择使用两节点feature之间的相关性作为临界矩阵。

## attention coefficients
GAT需要计算任意两节点之间的attention coefficient，公式如下  

$$
e_{ij} = a(\bold{W}\vec{h_i}, \bold{W}\vec{h_j}), \\
\alpha_{ij} = softmax_j(e_{ij}) = \frac{\exp{(e_{ij})}}{\sum_{k \in N_i}\exp{(e_{ik})}}
$$  

其中a是一个有多种选择的聚合方法，$$\bold{W}$$是一个可学习的矩阵  

文中提供的一种函数a的实现如下:  

$$
\alpha_{ij} =\frac{\exp{(LeakyReLU(\vec{a}^T[\bold{W}\vec{h_i} || \bold{W}\vec{h_j}]))}}{\sum_{k \in N_i}\exp{(LeakyReLU(\vec{a}^T[\bold{W}\vec{h_i} || \bold{W}\vec{h_k}]))}}
$$  

其中$$\vec{a}^T$$是一个可学习的向量，长度为2*in_channel  

## 聚合-传播过程  
通过计算attention coefficient，我们就得到了一个全新的边的权重矩阵，下面就使用这个权重矩阵进行其余操作。  

$$
\vec{h'_i} = {||}^K_{k=1} \sigma{(\sum_{j \in N_i} \alpha^k_{ij} \bold{W}^k \vec{h_j})}
$$  

即新得到的$$\vec{h'_i}$$是通过旧的节点邻居的feature加权相加后得到的。  
对于带多头注意力机制时，最后一层GAT不推荐使用concat操作，建议使用averaging操作  

$$
\vec{h'_i} = \sigma{(\frac{1}{K}\sum^K_{k=1}\sum_{j \in N_i} \alpha^k_{ij} \bold{W}^k \vec{h_j})}
$$  

## 和已有模型的比较
1. 时间复杂度更低，运行效率更高  
2. 和GCN相比，GAT对于有不同相似度的邻居节点附有不同的权重，而GCN只能使用相同的权重对邻居进行聚合。  
3. 可以在有向图上使用。
4. 在计算attention coefficient时的可学习参数是共享的，所以GAT是一种indicative的学习方法(即在模型训练时不需要知道整张图的所有节点)。
5. 和GraphSAGE相比，GAT不需要固定聚合邻居的个数，而是可以学习节点所有邻居的信息。而且，GAT不受GraphSAGE使用LSTM聚合时按不同顺序输入邻居节点得到的结果不同的问题。  
6. GAT使用节点的feature生成相似度，而不是使用图的结构信息。  
